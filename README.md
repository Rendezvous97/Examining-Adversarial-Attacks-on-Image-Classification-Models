# Network Architecture Specific Adversarial Attacks: * Variational Auto-Encoders Vs. Convolutional Neural Networks *

## I. Introduction
Current research on adversarial examples is largely focussed on deriving general defences against these attacks for all ML models irrespective of their architecture. In contrast to this methodology, we believe that each network architecture needs to be examined separately in order to make effective and specialized defensive capabilities. We must analyze the robustness of each architecture in isolation against different types of adversarial examples to understand the extent to which they are susceptible. Therefore, In this paper, we examine the extent to which Variational Auto-Encoders (Convolutional and Vanilla) and Convolutional Neural Networks (CNNs) are vulnerable to several gradient-based attacks on two types of datasets —  high pixel density (Labelled Faces in the Wild dataset) and low pixel density (MNIST). Our aim is to review the confidence of each attack, its validity and hence, the degree of effectiveness of the attack taking place for both types of architectures. Additionally, we also examine the role siamese networks could potentially play in creating more secure and robust systems. 

## II. Background
Deep neural networks are known for their high predictive capabilities, especially in comparison to humans. Yet, Szegedy et. al. (2014) discovered that this does not hold true in all contexts. They proved that several machine learning models, including state-of-the-art deep neural networks, are susceptible to adversarial examples, i.e, “ inputs to machine learning models designed by an adversary to cause an incorrect output” (Quin, et. al.). Given hat Machine Learning models are becoming ubiquitous, there is an increasing urgency for us to evaluate their susceptibility to these attacks, especially when the risks associated are high. In the context of image classification, these adversarial examples could lead to potentially harmful situations. For example, autonomous cars could be targetted to classify a sticker as a stop sign which can cause the car to brake unexpectedly (Goodfellow, et al. 2015). Similarly, face recognition and other biometric models, used to prevent unauthorized access to sensitive databases, could be fooled into allowing access via adversarial examples. In these cases within computer vision, the adversarial examples are usually images formed by making small perturbations to an example image through a gradient-based optimization approach.  

